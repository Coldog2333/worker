{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8378a74",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d272fa",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106a0be",
   "metadata": {},
   "source": [
    "### Import python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcabf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.utils.data\n",
    "import torch.utils.data.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28820f4a",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99817448",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=3\n",
    "MAX_SEQ_LEN=30\n",
    "EPOCH=5\n",
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521480e",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff27e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE='/home/ubuntu/worker/model/distilled_glove.42B.300d.txt' # distilled\n",
    "CV_FILE='/home/ubuntu/worker/model/alexnet-owt-4df8aa71.pth'\n",
    "ALEXNET_FILE='/home/ubuntu/worker/model/alexnet-owt-4df8aa71.pth'\n",
    "VGG_FILE='/home/ubuntu/worker/model/vgg19_bn-c79401a0.pth'\n",
    "flickr_root_dir = '/home/ubuntu/worker/dataset/flickr30k'\n",
    "flickr_caption_filename = flickr_root_dir + '/results_20130124.token'\n",
    "flickr_image_dir = flickr_root_dir + '/flickr30k-images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31e8cb",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d300f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embedding(glove_file):\n",
    "    # 0: pad, 1: unk\n",
    "    # dim: 300\n",
    "    glove_embeddings = [np.zeros(300), np.random.rand(300)]\n",
    "    word2id_dict = defaultdict(int)\n",
    "    id2word_dict = dict()\n",
    "    with open(glove_file) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            items = line.strip().split(' ')\n",
    "            word, embedding = items[0], np.array([float(v) for v in items[1:]])\n",
    "            glove_embeddings.append(embedding)\n",
    "            word2id_dict[word] = index\n",
    "            id2word_dict[index] = word\n",
    "    return np.array(glove_embeddings), word2id_dict, id2word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c61573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_caption(flickr_caption_filename, tokenize=False, numerize=False, word2id_dict=None):\n",
    "    assert (not (tokenize == False and numerize == True))   # 想要numerize，那么必须先tokenize\n",
    "\n",
    "    captions = defaultdict(list)\n",
    "    lengths = defaultdict(list)\n",
    "    with open(flickr_caption_filename) as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            image_filename, caption = line.strip().split('\\t')\n",
    "            image_filename = image_filename[:-2]\n",
    "            if tokenize:\n",
    "                caption = caption.strip().split(' ')\n",
    "            if numerize:\n",
    "                caption = [word2id_dict[w.lower()] for w in caption]\n",
    "                length = min(MAX_SEQ_LEN, len(caption))\n",
    "                caption = caption[:MAX_SEQ_LEN] + [0] * (MAX_SEQ_LEN - len(caption))\n",
    "            captions[image_filename].append(caption)\n",
    "            lengths[image_filename].append(length)\n",
    "    return captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc0f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((227, 227))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image / 127.5\n",
    "    image = image - 1.0  # 归一化到[-1, 1]之间\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41204783",
   "metadata": {},
   "source": [
    "#### Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ab001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr_Dataset_MemoryFriendly(torch.utils.data.Dataset):\n",
    "    def __init__(self, flickr_caption_filename, flickr_image_dir, word2id_dict=None):\n",
    "        captions_dict, lengths_dict = load_caption(flickr_caption_filename, tokenize=True, numerize=True, word2id_dict=word2id_dict)\n",
    "        self.flickr_image_dir = flickr_image_dir\n",
    "        image_filenames = os.listdir(flickr_image_dir)\n",
    "\n",
    "        self.captions, self.images, self.lengths = [], [], []\n",
    "        for filename in image_filenames:\n",
    "            for caption, length in zip(captions_dict[filename], lengths_dict[filename]):\n",
    "                self.captions.append(caption)\n",
    "                self.lengths.append(length)\n",
    "                self.images.append(os.path.join(self.flickr_image_dir, filename))\n",
    "\n",
    "        self.pos_captions = torch.tensor(self.captions, dtype=torch.long)\n",
    "        self.pos_lengths = torch.tensor(self.lengths, dtype=torch.int64)\n",
    "        self.neg_captions, self.neg_lengths = [], []\n",
    "        for _ in range(K):\n",
    "            index = list(range(self.pos_captions.shape[0]))\n",
    "            random.shuffle(index)\n",
    "            self.neg_captions.append(self.pos_captions[index])\n",
    "            self.neg_lengths.append(self.pos_lengths[index])\n",
    "        self.neg_captions = torch.stack(self.neg_captions, dim=1)\n",
    "        self.neg_lengths = torch.stack(self.neg_lengths, dim=1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        input_image = load_single_image(self.images[i])\n",
    "        input_image = torch.tensor(input_image, dtype=torch.float).squeeze().permute(2, 0, 1)\n",
    "        return input_image, self.pos_captions[i], self.pos_lengths[i], self.neg_captions[i], self.neg_lengths[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a85801",
   "metadata": {},
   "source": [
    "### Define neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86804378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, output_feat=False):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.output_feat = output_feat\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        Args:\n",
    "            x (Tensor): input tensor\n",
    "        Returns:\n",
    "            output (Tensor): output tensor\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\n",
    "        if self.output_feat:\n",
    "            return x\n",
    "        else:\n",
    "            return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "284d9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Representer(nn.Module):\n",
    "    def __init__(self, embedding_matrix, use_rnn=True):\n",
    "        super(Text_Representer, self).__init__()\n",
    "        self.use_rnn = use_rnn\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_num, self.embed_dim = self.embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(self.vocab_num, self.embed_dim)\n",
    "        self.embedding.weight = embedding_matrix\n",
    "\n",
    "        self.gru = nn.GRU(input_size=self.embed_dim, hidden_size=256 * 6, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=256 * 6, out_features=256 * 6 * 6)\n",
    "\n",
    "    def forward(self, sentence, length):\n",
    "        x = self.embedding(sentence)\n",
    "        if self.use_rnn:\n",
    "            # x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "            x = torch.nn.utils.rnn.pack_padded_sequence(x, length, batch_first=True, enforce_sorted=False) \n",
    "            _, x = self.gru(x)\n",
    "            x = self.linear(x[0])\n",
    "        else:\n",
    "            x = torch.sum(x, dim=1)\n",
    "            x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77c73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Modal_Retriever(nn.Module):\n",
    "    def __init__(self, embedding_matrix, cv_weight_file=None):\n",
    "        super(Cross_Modal_Retriever, self).__init__()\n",
    "        self.cv_net = AlexNet(output_feat=True)\n",
    "        if cv_weight_file:\n",
    "            self.cv_net.load_state_dict(torch.load(cv_weight_file))\n",
    "        self.nlp_net = Text_Representer(embedding_matrix=embedding_matrix)\n",
    "\n",
    "    def forward(self, image, pos_caption, pos_length, neg_captions, neg_lengths):\n",
    "        cv_feats = self.cv_net(image)\n",
    "        nlp_pos_feats = self.nlp_net(pos_caption, pos_length.view(-1))\n",
    "        neg_pos_feats = self.nlp_net(neg_captions.view(-1, neg_captions.shape[2]), neg_lengths.view(-1))\n",
    "        neg_pos_feats = neg_pos_feats.view(neg_captions.shape[0], neg_captions.shape[1], -1)\n",
    "        # similarity = torch.diagonal(torch.matmul(nlp_feats, cv_feats.T))\n",
    "        return cv_feats, nlp_pos_feats, neg_pos_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a62e16",
   "metadata": {},
   "source": [
    "### Define loss function for metric learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b05d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    '''\n",
    "    Compute normal triplet loss or soft margin triplet loss given triplets\n",
    "    '''\n",
    "    def __init__(self, margin=None):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        if self.margin is None:  # if no margin assigned, use soft-margin\n",
    "            self.Loss = nn.SoftMarginLoss()\n",
    "        else:\n",
    "            self.Loss = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        if self.margin is None:\n",
    "            num_samples = anchor.shape[0]\n",
    "            y = torch.ones((num_samples, 1)).view(-1)\n",
    "            if anchor.is_cuda: y = y.cuda()\n",
    "            ap_dist = torch.norm(anchor-pos, 2, dim=1).view(-1)\n",
    "            an_dist = torch.norm(anchor-neg, 2, dim=1).view(-1)\n",
    "            loss = self.Loss(an_dist - ap_dist, y)\n",
    "        else:\n",
    "            loss = self.Loss(anchor, pos, neg)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc4310",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee8a2aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158915/158915 [00:01<00:00, 98051.40it/s]\n"
     ]
    }
   ],
   "source": [
    "##### debug 词向量部分\n",
    "embedding_matrix, word2id_dict, _ = load_glove_embedding(GLOVE_FILE)\n",
    "# print(embedding_matrix.shape)\n",
    "##### debug部分\n",
    "retriever = Cross_Modal_Retriever(embedding_matrix=nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float),\n",
    "                                                                requires_grad=True),\n",
    "                                    cv_weight_file=CV_FILE).cuda()\n",
    "# train_dataset = Flickr_Dataset(flickr_caption_filename, flickr_image_dir, word2id_dict=word2id_dict)\n",
    "# test_dataset = Flickr_Dataset(flickr_caption_filename, flickr_image_dir, word2id_dict=word2id_dict)\n",
    "all_dataset = Flickr_Dataset(flickr_caption_filename, flickr_image_dir, word2id_dict=word2id_dict)\n",
    "\n",
    "dataset_size = len(all_dataset)\n",
    "train_size, test_size = int(0.8 * dataset_size), int(0.2 * dataset_size)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(all_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84920dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b2a75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = TripletLoss(margin=None)\n",
    "optimizer = torch.optim.Adam(params=retriever.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ea37c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step 1: loss is 2.0792458057403564\n",
      "    step 51: loss is 2.0498646310731474\n",
      "    step 101: loss is 1.974066696544685\n",
      "    step 151: loss is 1.887848048810138\n",
      "    step 201: loss is 1.8166883970374492\n",
      "    step 251: loss is 1.752949283892415\n",
      "    step 301: loss is 1.704050165870261\n",
      "    step 351: loss is 1.6602387197336919\n",
      "    step 401: loss is 1.618179939036952\n",
      "    step 451: loss is 1.5801041681327734\n",
      "    step 501: loss is 1.5460684075802862\n",
      "    step 551: loss is 1.5180851604025507\n",
      "    step 601: loss is 1.4891209725333927\n",
      "    step 651: loss is 1.4636205660033337\n",
      "    step 701: loss is 1.4391313158666525\n",
      "    step 751: loss is 1.414180849347705\n",
      "    step 801: loss is 1.392053521453367\n",
      "    step 851: loss is 1.372014387303878\n",
      "    step 901: loss is 1.3520850143607264\n",
      "    step 951: loss is 1.3332064521550882\n",
      "EPOCH 1: Loss is 1.3173130331504752\n",
      "acc: 0.6747317748481892\n",
      "    step 1: loss is 0.7976595163345337\n",
      "    step 51: loss is 0.8621252576510111\n",
      "    step 101: loss is 0.854474663734436\n",
      "    step 151: loss is 0.8495332176322179\n",
      "    step 201: loss is 0.8422941298627141\n",
      "    step 251: loss is 0.8324174797867399\n",
      "    step 301: loss is 0.8200225236011898\n",
      "    step 351: loss is 0.8182627533236121\n",
      "    step 401: loss is 0.8118234871331593\n",
      "    step 451: loss is 0.8071051427636072\n",
      "    step 501: loss is 0.8013067763603614\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-17f82658f757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    total_loss = []\n",
    "    retriever.train()\n",
    "    for step, (image, pos_caption, pos_length, neg_captions, neg_lengths) in enumerate(train_dataloader):\n",
    "        [image, pos_caption, neg_captions] = [tensor.cuda() for tensor in [image, pos_caption,  neg_captions]]\n",
    "        cv_feats, nlp_pos_feat, nlp_neg_feats = retriever(image, pos_caption, pos_length, neg_captions, neg_lengths)\n",
    "\n",
    "        print_losses = []\n",
    "\n",
    "        loss = loss_function(cv_feats, nlp_pos_feat, nlp_neg_feats[:, 0, :])\n",
    "        # total_loss = loss\n",
    "        # print_losses.append(loss.item())\n",
    "        for i in range(1, K):\n",
    "            loss += loss_function(cv_feats, nlp_pos_feat, nlp_neg_feats[:, i, :])\n",
    "            # total_loss += loss\n",
    "            # print_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        if step % 50 == 0:\n",
    "            print('    step %s: loss is %s' % (step + 1, np.mean(total_loss)))\n",
    "\n",
    "    print('EPOCH %s: Loss is %s' % ((epoch + 1), np.mean(total_loss)))\n",
    "\n",
    "    results = []\n",
    "    for step, (image, pos_caption, pos_length, neg_captions, neg_lengths) in enumerate(test_dataloader):\n",
    "        [image, pos_caption, neg_captions] = [tensor.cuda() for tensor in [image, pos_caption,  neg_captions]]\n",
    "        retriever.eval()\n",
    "        cv_feats, nlp_pos_feat, nlp_neg_feats = retriever(image, pos_caption, pos_length, neg_captions, neg_lengths)\n",
    "\n",
    "        dists = [torch.norm(cv_feats - nlp_pos_feat, 2, dim=1).view(-1).item()]\n",
    "        for i in range(nlp_neg_feats.shape[1]):\n",
    "            dist = torch.norm(cv_feats - nlp_neg_feats[:, i, :], 2, dim=1).view(-1)\n",
    "            dists.append(dist.item())\n",
    "\n",
    "        if np.argmin(dists) != 0:\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "\n",
    "    print(\"acc: %s\" % (np.mean(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893e90c",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667c76e",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84be2b",
   "metadata": {},
   "source": [
    "### Define other neural networks\n",
    "#### VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712aaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=1000, output_feat=True, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.output_feat = output_feat\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.output_feat:\n",
    "            return x\n",
    "        else:\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def vgg19_bn(pretrained=False, cv_weight_file=None, **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['E'], batch_norm=True), output_feat=True, **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(torch.load(cv_weight_file))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df21f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Modal_Retriever_v2(nn.Module):\n",
    "    def __init__(self, embedding_matrix, alexnet_weight_file, vgg19_weight_file=None):\n",
    "        super(Cross_Modal_Retriever_v2, self).__init__()\n",
    "        self.alexnet = AlexNet(output_feat=True)\n",
    "        self.alexnet.load_state_dict(torch.load(alexnet_weight_file))\n",
    "        self.vgg19 = vgg19_bn(pretrained=True, cv_weight_file=vgg19_weight_file)\n",
    "        self.vgg_linear = nn.Linear(25088, 256 * 6 * 6)\n",
    "        \n",
    "        self.glove_net = Text_Representer(embedding_matrix=embedding_matrix)\n",
    "        self.rnn_net = Text_Representer(embedding_matrix=embedding_matrix, use_rnn=True)\n",
    "\n",
    "    def forward(self, image, pos_caption, pos_length, neg_captions, neg_lengths):\n",
    "        alexnet_feat = self.alexnet(image)\n",
    "        vgg19_feat = self.vgg_linear(self.vgg19(image))\n",
    "        print(vgg19_feat.shape, alexnet_feat.shape)\n",
    "        cv_feats = torch.cat([alexnet_feat, vgg19_feat], dim=-1)\n",
    "        \n",
    "        glove_pos_feats = self.glove_net(pos_caption, pos_length.view(-1))\n",
    "        rnn_pos_feats = self.rnn_net(pos_caption, pos_length.view(-1))\n",
    "        nlp_pos_feats = torch.cat([glove_pos_feats, rnn_pos_feats], dim=-1)   \n",
    "        \n",
    "        glove_neg_feats = self.glove_net(neg_captions.view(-1, neg_captions.shape[2]), neg_lengths.view(-1))\n",
    "        glove_neg_feats = glove_neg_feats.view(neg_captions.shape[0], neg_captions.shape[1], -1)\n",
    "        rnn_neg_feats = self.rnn_net(neg_captions.view(-1, neg_captions.shape[2]), neg_lengths.view(-1))\n",
    "        rnn_neg_feats = rnn_neg_feats.view(neg_captions.shape[0], neg_captions.shape[1], -1)\n",
    "        nlp_neg_feats = torch.cat([glove_neg_feats, rnn_neg_feats], dim=-1) \n",
    "        \n",
    "        return cv_feats, nlp_pos_feats, nlp_neg_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ed0854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158915/158915 [00:01<00:00, 134404.95it/s]\n"
     ]
    }
   ],
   "source": [
    "##### debug 词向量部分\n",
    "embedding_matrix, word2id_dict, _ = load_glove_embedding(GLOVE_FILE)\n",
    "# print(embedding_matrix.shape)\n",
    "##### debug部分\n",
    "retriever = Cross_Modal_Retriever_v2(embedding_matrix=nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float),\n",
    "                                                                requires_grad=True),\n",
    "                                     alexnet_weight_file=ALEXNET_FILE,\n",
    "                                     vgg19_weight_file=VGG_FILE).cuda()\n",
    "# train_dataset = Flickr_Dataset(flickr_caption_filename, flickr_image_dir, word2id_dict=word2id_dict)\n",
    "# test_dataset = Flickr_Dataset(flickr_caption_filename, flickr_image_dir, word2id_dict=word2id_dict)\n",
    "all_dataset = Flickr_Dataset_MemoryFriendly(flickr_caption_filename, flickr_image_dir, word2id_dict=word2id_dict)\n",
    "\n",
    "dataset_size = len(all_dataset)\n",
    "train_size, test_size = int(0.8 * dataset_size), int(0.2 * dataset_size)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(all_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee19d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=2)\n",
    "loss_function = TripletLoss(margin=None)\n",
    "optimizer = torch.optim.Adam(params=retriever.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9924f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9216]) torch.Size([1, 9216])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 7.43 GiB total capacity; 5.36 GiB already allocated; 598.94 MiB free; 6.21 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-17f82658f757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 7.43 GiB total capacity; 5.36 GiB already allocated; 598.94 MiB free; 6.21 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    total_loss = []\n",
    "    retriever.train()\n",
    "    for step, (image, pos_caption, pos_length, neg_captions, neg_lengths) in enumerate(train_dataloader):\n",
    "        [image, pos_caption, neg_captions] = [tensor.cuda() for tensor in [image, pos_caption,  neg_captions]]\n",
    "        cv_feats, nlp_pos_feat, nlp_neg_feats = retriever(image, pos_caption, pos_length, neg_captions, neg_lengths)\n",
    "\n",
    "        print_losses = []\n",
    "\n",
    "        loss = loss_function(cv_feats, nlp_pos_feat, nlp_neg_feats[:, 0, :])\n",
    "        # total_loss = loss\n",
    "        # print_losses.append(loss.item())\n",
    "        for i in range(1, K):\n",
    "            loss += loss_function(cv_feats, nlp_pos_feat, nlp_neg_feats[:, i, :])\n",
    "            # total_loss += loss\n",
    "            # print_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        if step % 50 == 0:\n",
    "            print('    step %s: loss is %s' % (step + 1, np.mean(total_loss)))\n",
    "\n",
    "    print('EPOCH %s: Loss is %s' % ((epoch + 1), np.mean(total_loss)))\n",
    "\n",
    "    results = []\n",
    "    for step, (image, pos_caption, pos_length, neg_captions, neg_lengths) in enumerate(test_dataloader):\n",
    "        [image, pos_caption, neg_captions] = [tensor.cuda() for tensor in [image, pos_caption,  neg_captions]]\n",
    "        retriever.eval()\n",
    "        cv_feats, nlp_pos_feat, nlp_neg_feats = retriever(image, pos_caption, pos_length, neg_captions, neg_lengths)\n",
    "\n",
    "        dists = [torch.norm(cv_feats - nlp_pos_feat, 2, dim=1).view(-1).item()]\n",
    "        for i in range(nlp_neg_feats.shape[1]):\n",
    "            dist = torch.norm(cv_feats - nlp_neg_feats[:, i, :], 2, dim=1).view(-1)\n",
    "            dists.append(dist.item())\n",
    "\n",
    "        if np.argmin(dists) != 0:\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "\n",
    "    print(\"acc: %s\" % (np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f819ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4915ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468bc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923c43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca1940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
